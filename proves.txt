Big Data

Big data, macrodatos,1datos masivos, inteligencia de datos o datos a gran escala es un concepto que hace referencia a conjuntos de datos tan grandes y complejos como para que aplicaciones informáticas tradicionales de procesamiento de datos puedan tratarlos adecuadamente. Por ende, los procedimientos usados para encontrar patrones repetitivos dentro de esos datos son más sofisticados y requieren software especializado. En textos científicos en español con frecuencia se usa directamente el término en inglés big data, tal como aparece en el ensayo de Viktor Schönberger: La revolución de los datos masivos.

Arquitectura

Los repositorios de big data han existido en muchas formas, a menudo creadas por corporaciones con una necesidad especial. Históricamente, los proveedores comerciales ofrecían sistemas de administración de bases de datos paralelos para big data a partir de la década de 1990. Durante muchos años, WinterCorp publicó un informe de base de datos más grande.31​

Teradata Corporation en 1984, comercializó el sistema de procesamiento paralelo DBC 1012. Los sistemas Teradata fueron los primeros en almacenar y analizar 1 terabyte de datos en 1992. Los discos duros eran de 2,5 GB en 1991, por lo que la definición de big data evoluciona continuamente según la Ley de Kryder. Teradata instaló el primer sistema basado en RDBMSde clase petabyte en 2007. A partir de 2017, hay unas pocas docenas de bases de datos relacionales de Teradata de clase Petabyte instaladas, la mayor de las cuales excede de 50 PB. Los sistemas hasta 2008 eran datos relacionales estructurados al 100%. Desde entonces, Teradata ha agregado tipos de datos no estructurados, incluidos XML, JSONy Avro.

En 2000, Seisint Inc. (ahora LexisNexis Group) desarrolló un marco de intercambio de archivos distribuido basado en C++ para el almacenamiento y consultas de datos. El sistema almacena y distribuye datos estructurados, semiestructurados y no estructurados en varios servidores. Los usuarios pueden crear consultas en un dialecto de C++ llamado ECL. ECL utiliza un método de "aplicar esquema en lectura" para inferir la estructura de los datos almacenados cuando se consulta, en lugar de cuando se almacena. En 2004, LexisNexis adquirió Seisint Inc.32​ y en 2008 adquirió ChoicePoint, Inc.33​y su plataforma de procesamiento paralelo de alta velocidad. Las dos plataformas se fusionaron en sistemas HPCC (o cluster de computación de alto rendimiento) y en 2011, HPCC fue de código abierto bajo la licencia Apache v2.0. Quantcast File Systemestuvo disponible aproximadamente al mismo tiempo.34​

El CERN y otros experimentos de física han recopilado grandes conjuntos de datos durante muchas décadas, generalmente analizados a través de computadoras de alto rendimiento (supercomputadores) en lugar de las arquitecturas de mapas reducidos de productos, que generalmente se refieren al movimiento actual de "big data".

En 2004, Google publicó un documento sobre un proceso llamado MapReduceque utiliza una arquitectura similar. El concepto MapReduce proporciona un modelo de procesamiento en paralelo, y se lanzó una implementación asociada para procesar grandes cantidades de datos. Con MapReduce, las consultas se dividen y distribuyen a través de nodos paralelos y se procesan en paralelo (el paso del Mapa). Los resultados se recopilan y se entregan (el paso Reducir). El marco fue muy exitoso, por lo que otros quisieron replicar el algoritmo. Por lo tanto, una implementación del marco MapReduce fue adoptada por un proyecto de código abierto Apache llamado Hadoop. 35​Apache Spark se desarrolló en 2012 en respuesta a las limitaciones del paradigma MapReduce, ya que agrega la capacidad de configurar muchas operaciones (no solo el mapa seguido de la reducción).

MIKE2.0 es un enfoque abierto para la administración de la información que reconoce la necesidad de revisiones debido a las implicaciones de big data identificadas en un artículo titulado "Oferta de soluciones de Big Data".36​La metodología aborda el manejo de big data en términos de permutaciones útiles de fuentes de datos, complejidad en interrelaciones y dificultad para eliminar (o modificar) registros individuales.37​

Los estudios de 2012 mostraron que una arquitectura de capas múltiples es una opción para abordar los problemas que presenta el big data. Una arquitectura paralela distribuida distribuye datos entre múltiples servidores; estos entornos de ejecución paralela pueden mejorar drásticamente las velocidades de procesamiento de datos. Este tipo de arquitectura inserta datos en un DBMS paralelo, que implementa el uso de los marcos MapReduce y Hadoop. Este tipo de marco busca hacer que el poder de procesamiento sea transparente para el usuario final mediante el uso de un servidor de aplicaciones para el usuario.38​

El análisis de Big Data para aplicaciones de fabricación se comercializa como una arquitectura 5C (conexión, conversión, cibernética, cognición y configuración).39​

El lago de datos permite que una organización cambie su enfoque del control centralizado a un modelo compartido para responder a la dinámica cambiante de la administración de la información. Esto permite una segregación rápida de datos en el lago de datos, lo que reduce el tiempo de sobrecarga. 40​41​ ​
